# 什么架构设计？

	站在老板的角度来说，技术的引入和落地，需要多少人力成本和资金投入（一个字，就是钱），它能不能带来收益，也就是商业价值。
	站在架构师的角度来说，作为一个架构师，他的职责就是技术决策和成本核算；
	也就是说，技术要服务于业务，技术的选项和落地，到底需要多少技术成本和人力成本，能否创造价值。

	架构设计遵循3大规则：
		 1.技术是需要业务作为支撑。
		 2.技术选型和落地，需要多少成本。
		 3.没有稳定性，一切都免谈。

# 高并发系统设计的方法和思考

	设计高并发系统的目的：
		将业务规则采用技术实现，做到系统的高可用，高并发，高性能，从而保证系统的稳定性和可用性。
# 通用设计方法

	1. Scale-out（横向扩展）：分而治之是一种常见的高并发系统设计方法，采用分布式部署的方式把流量分流开，让每个服务器都承担一部分并发和流量。
	2. 缓存：使用缓存来提高系统的性能，就好比用“拓宽河道”的方式抵抗高并发大流量的冲击。
	3. 异步：在某些场景下，未处理完成之前我们可以让请求先返回，在数据准备好之后再通知请求方，这样可以在单位时间内处理更多的请求。

	横向拓展：主要是分布式部署，流量切分
	缓存： 采用缓存提高并发，缩短响应时间
	异步： 服务逻辑采用异步解耦，采用异步的方式，后端处理时会把请求丢到消息队列中，同时快速响应用户，告诉用户我们正在排队处理，然后释放出资源来处理更多的请求
		  调用方不需要等待方法逻辑执行完成就可以返回执行其他的逻辑，在被调用方法执行完毕后再通过回调、事件通知等方式将结果反馈给调用方。

# 性能优化的4大指导原则

	1.不要过早优化，优化的前提：要结合具体业务场景，具体分析
	2.系统优化遵循“二八原则”，用20%的成本解决80%的性能问题，需要抓住主要矛盾，优先优化主要的性能瓶颈点
	3.性能优化需要有数据支撑，要时刻了解响应时间的减少了多少和吞吐量的提升了多少
	4.性能优化是随着业务的复杂性，持续优化（罗马不是一天建成的，要不断寻找性能出现的瓶颈，持续优化）

# 性能优化度量指标

	1.请求平均响应时间
	2.响应时间的最大值和最小值
	3.分位值，p90,p95,p75不同区间响应时间是多少

# 高并发下的性能优化

	1.提高系统的处理核心数，增加系统并行处理能力（在资源利用，吞吐量，响应时间提升的时候，找到拐点）
	2.减少单次任务响应时间
		对cpu密集型选择高效的算法，提升运算能力；
		对于i/o密集型（比如db,cache,web系统等），减少i/o等待；
			比如： 
				数据库访问慢，是否有锁表，是否有全表扫描，索引是否合理，是否有大sql（一般是join操作）
				业务逻辑是否有缓存，缓存是否击穿或穿透
				网络是否有延迟，网卡是否有丢包情况等

# 系统高可用
	
	系统具备较高的无故障运行的能力
	通常来讲，一个高并发大流量的系统，系统出现故障比系统性能低更损伤用户的使用体验。
	想象一下，一个日活用户过百万的系统，一分钟的故障可能会影响到上千的用户。
	而且随着系统日活的增加，一分钟的故障时间影响到的用户数也随之增加，系统对于可用性的要求也会更高。

	度量指标：
		MTBF（Mean Time Between Failure）是平均故障间隔的意思，代表两次故障的间隔时间，也就是系统正常运转的平均时间。这个时间越长，系统稳定性越高。
		MTTR（Mean Time To Repair）表示故障的平均恢复时间，也可以理解为平均故障时间。这个值越小，故障对于用户的影响越小。

		可用性与 MTBF 和 MTTR 的值息息相关，我们可以用下面的公式表示它们之间的关系：
			Availability = MTBF / (MTBF + MTTR)
		这个公式计算出的结果是一个比例，而这个比例代表着系统的可用性。一般来说，我们会使用几个九来描述系统的可用性。

		系统可用性		年故障时间		日故障时间
		90%	1个9			36.5d 			2.4h
		99% 2个9     	3.65d 			74.4min
		99.9% 3个9      8h 				7.44min
		99.99% 4个9     52min   			8.6s
		99.999% 5个9    5min            0.86s
		99.9999% 6个g   32s 				86ms

		一般来说1个9，2个9很容易达到；故障时间越短，故障恢复时间越短，系统就越稳定；
		核心业务系统的可用性，需要达到四个九，非核心系统的可用性最多容忍到三个九。
		在实际工作中，你可能听到过类似的说法，只是不同级别，不同业务场景的系统对于可用性要求是不一样的；

# 高可用设计思路
	
	一个成熟系统的可用性需要从系统设计和系统运维两方面来做保障，两者共同作用，缺一不可；

	系统设计：
		在做系统设计的时候，要把发生故障作为一个重要的考虑点，预先考虑如何自动化地发现故障，发生故障之后要如何解决;
		系统设计过程中，要保证服务稳定性得到保证；
			比如：故障检测（一般是心跳检测），故障自动转移，超时控制，服务降级，服务限流；
			故障检测的方法： 使用最广泛的故障检测机制是“心跳”。你可以在客户端上定期地向主节点发送心跳包，也可以从备份节点上定期发送心跳包。
						   当一段时间内未收到心跳包，就可以认为主节点已经发生故障，可以触发选主的操作。
						   选主的结果需要在多个备份节点上达成一致，所以会使用某一种分布式一致性算法，比方说 Paxos，Raft；
			超时控制：
				超时控制实际上就是不让请求一直保持，而是在经过一定时间之后让请求失败，释放资源给接下来的请求使用。
				这对于用户来说是有损的，但是却是必要的，因为它牺牲了少量的请求却保证了整体系统的可用性。
			服务降级：
				降级是为了保证核心服务的稳定而牺牲非核心服务的做法。
				比方说我们发一条微博会先经过反垃圾服务检测，检测内容是否是广告，通过后才会完成诸如写数据库等逻辑；
			服务限流：
				它通过对并发的请求进行限速来保护系统；
				比如对于 Web 应用，我限制单机只能处理每秒 1000 次的请求，超过的部分直接返回错误给客户端。
				虽然这种做法损害了用户的使用体验，但是它是在极端并发下的无奈之举，是短暂的行为，因此是可以接受的。


	系统运维：
		运维层面，要做好灰度发布，故障演练，快速响应

		灰度发布： 系统的变更不是一次性地推到线上的，而是按照一定比例逐步推进的。一般情况下，灰度发布是以机器维度进行的。
				  比方说，我们先在 10% 的机器上进行变更，同时观察 Dashboard 上的系统性能指标以及错误日志。
				  如果运行了一段时间之后系统指标比较平稳并且没有出现大量的错误日志，那么再推动全量变更。
				  灰度发布给了开发和运维同学绝佳的机会，让他们能在线上流量上观察变更带来的影响，是保证系统高可用的重要关卡。

		故障演练： 对系统进行一些破坏性的手段，观察在出现局部故障时，整体的系统表现是怎样的，从而发现系统中存在的，潜在的可用性问题。
				  比如一个复杂的高并发系统依赖了太多的组件，比方说磁盘，数据库，网卡等，这些组件随时随地都可能会发生故障，而一旦它们发生故障，
				  会不会如蝴蝶效应一般造成整体服务不可用呢？我们并不知道，因此，故障演练尤为重要。
				  对于故障演练，一般建议搭建一套和线上部署结构一模一样的线下系统，然后在这套系统上做故障演练，从而避免对生产系统造成影响。
   
# 开发和运维眼中的高可用的方法
	
	从开发和运维角度上来看，提升可用性的方法是不同的：
		对于开发来说：
			开发注重的是如何处理故障，关键词是冗余和取舍。冗余指的是有备用节点，集群来顶替出故障的服务。
			比如文中提到的故障转移，还有多活架构等等；取舍指的是丢卒保车，保障主体服务的安全

		对于运维来说：
			从运维角度来看则更偏保守，注重的是如何避免故障的发生。
			比如更关注变更管理以及如何做故障的演练，当出现系统出现故障，怎么快速响应解决系统故障。

	只有开发和运维两个角度结合起来，才可以真正保证系统的高可用；
	与此同时，提供系统的高可用性，还需要考虑投入的成本，系统优化要合理，要考虑满足高可用的同时，是否可以带来收益，维持系统的稳定性；

# 系统的高可拓展性

	从架构设计上来说，高可扩展性是一个设计的指标，它表示可以通过增加机器的方式来线性提高系统的处理能力，从而承担更高的流量和并发。

	在单机系统中通过增加处理核心的方式，来增加系统的并行处理能力，但这个方式并不总奏效。
	因为当并行的任务数较多时，系统会因为争抢资源而达到性能上的拐点，系统处理能力不升反降。
	而对于由多台机器组成的集群系统来说也是如此。集群系统中，不同的系统分层上可能存在一些“瓶颈点”，这些瓶颈点制约着系统的横向扩展能力。
	比方说，你系统的流量是每秒 1000 次请求，对数据库的请求量也是每秒 1000 次。
	如果流量增加 10 倍，虽然系统可以通过扩容正常服务，数据库却成了瓶颈。
	再比方说，单机网络带宽是 50Mbps，那么如果扩容到 30 台机器，前端负载均衡的带宽就超过了千兆带宽的限制，也会成为瓶颈点。

	其实，无状态的服务和组件更易于扩展，而像 MySQL 这种存储服务是有状态的，就比较难以扩展。
	因为向存储集群中增加或者减少机器时，会涉及大量数据的迁移，而一般传统的关系型数据库都不支持。
	这就是为什么提升系统扩展性会很复杂的主要原因。

	除此之外，从例子中你可以看到，我们需要站在整体架构的角度，而不仅仅是业务服务器的角度来考虑系统的扩展性 。
	所以说，数据库、缓存、依赖的第三方、负载均衡、交换机带宽等等都是系统扩展时需要考虑的因素。
	我们要知道系统并发到了某一个量级之后，哪一个因素会成为我们的瓶颈点，从而针对性地进行扩展。

# 高可扩展性的设计思路
	
	服务拆分：将复杂的系统，拆分为小模块，将复杂的问题简单化
		拆分是提升系统扩展性最重要的一个思路，它会把庞杂的系统拆分成独立的，有单一职责的模块。
		相对于大系统来说，考虑一个一个小模块的扩展性当然会简单一些，将复杂的问题简单化，这是我们的思路；

	1.存储层的拓展性：
		无论是存储的数据量，还是并发访问量，不同的业务模块之间的量级相差很大；
		比如说成熟社区中，关系的数据量是远远大于用户数据量的，但是用户数据的访问量却远比关系数据要大。
		所以假如存储目前的瓶颈点是容量，那么我们只需要针对关系模块的数据做拆分就好了，而不需要拆分用户模块的数据。
		所以存储拆分首先考虑的维度是业务维度。
		在垂直方向，按照业务纬度，将存储层的数据库拆分；
		比如用户库，内容库，订单库，商品库，库存库，评论库，商品优惠库等等；这么做还能隔离故障，某一个库“挂了”不会影响到其它的数据库。

		按照业务拆分服务和库存在的问题：
			按照业务拆分，在一定程度上提升了系统的扩展性，但系统运行时间长了之后，
			单一的业务数据库在容量和并发请求量上仍然会超过单机的限制。这时，我们就需要针对数据库做第二次拆分。

		数据按照一定的特征水平拆分：
			这次拆分是按照数据特征做水平的拆分，比如说我们可以给用户库增加两个节点，然后按照某些算法将用户的数据拆分到这三个库里面；

		水平拆分存在的问题：
			水平拆分之后，我们就可以让数据库突破单机的限制了。但这里要注意，我们不能随意地增加节点。
			因为一旦增加节点就需要手动地迁移数据，成本还是很高的。
			所以基于长远的考虑，我们最好一次性增加足够的节点以避免频繁的扩容。

		数据库按照业务和数据维度拆分存在的问题：
			拆分后，我们尽量不要使用事务。
			因为当一个事务中同时更新不同的数据库时，需要使用二阶段提交来协调所有数据库，要么全部更新成功，要么全部更新失败。
			这个协调的成本会随着资源的扩展不断升高，最终达到无法承受的程度。

	2.业务层的扩展性
		我们一般会从三个维度考虑业务层的拆分方案，它们分别是：业务维度，重要性维度和请求来源维度。
		1）我们需要把相同业务的服务拆分成单独的业务池，每个业务依赖自己的数据库资源，数据层面相互隔离；
			比方说社区系统中，我们可以按照业务的维度拆分成用户池、内容池、关系池、评论池、点赞池和搜索池；
			每个业务依赖独自的数据库资源，不会依赖其它业务的数据库资源。这样当某一个业务的接口成为瓶颈时，
			我们只需要扩展业务的池子，以及确认上下游的依赖方就可以了，这样就大大减少了扩容的复杂度。

		2）根据业务接口的重要程度，把业务分为核心池和非核心池，按照业务重要程度划分，提供业务层面的拓展性。
			打个比方，就关系池而言，关注、取消关注接口相对重要一些，可以放在核心池里面；
			拉黑和取消拉黑的操作就相对不那么重要，可以放在非核心池里面。
			这样，我们可以优先保证核心池的性能，当整体流量上升时优先扩容核心池，降级部分非核心池的接口，从而保证整体系统的稳定性。

		3）根据接入客户端类型的不同做业务池的拆分；
	    	   比如，服务于客户端接口的业务可以定义为外网池，服务于小程序或者h5页面的业务可以定义为H5池，服务于内部其它部门的业务可以定义为内网池等等。

# 对于未拆分的服务思考
	
	未做拆分的系统虽然可扩展性不强，但是却足够简单，无论是系统开发还是运行维护都不需要投入很大的精力。
	拆分之后，需求开发需要横跨多个系统多个小团队，排查问题也需要涉及多个系统，运行维护上，可能每个子系统都需要有专人来负责，
	对于团队是一个比较大的考验，也就是人力成本，这个考验是我们必须要经历的一个大坎，需要我们做好准备。

# 数据库高可用设计

	1. 读写分离策略，将读写的流量进行切分，降低db的压力;
	2. 减少频繁创建数据库连接，一般采用数据库连接池和线程池，池化技术解决;
	3.数据库和NoSQL如何互补;

# 查询请求增加，如何做主从分离

	主从读写分离，大部分系统读多写少，将读写流量分开；
		一般来说在主从读写分离机制中，我们将一个数据库的数据拷贝为一份或者多份，并且写入到其它的数据库服务器中，原始的数据库我们称为主库，
		主要负责数据的写入，拷贝的目标数据库称为从库，主要负责支持数据查询。
		可以看到，主从读写分离有两个技术上的关键点：
			1） 一个是数据的拷贝，我们称为主从复制；
			2） 在主从分离的情况下，我们如何屏蔽主从分离带来的访问数据库方式的变化，让开发同学像是在使用单一数据库一样，一般采用数据库中间件或交给db底层去完成（比如tidb,newsql这样的产品）
		1.主从复制：
			MySQL 的主从复制是依赖于 binlog 的，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上二进制日志文件。
			主从复制就是将 binlog 中的数据从主库传输到从库上，一般这个过程是异步的，即主库上的操作不会等待 binlog 同步的完成。

			首先从库在连接到主节点时会创建一个 IO 线程，用以请求主库更新的 binlog，并且把接收到的 binlog 信息写入一个叫做 relay log 的日志文件
			最终实现主从的一致性。这是一种比较常见的主从复制方式。
			在这个方案中，使用独立的 log dump 线程是一种异步的方式，可以避免对主库的主体更新流程产生影响，而从库在接收到信息后并不是写入从库的存储中，
			是写入一个 relay log，是避免写入从库实际存储会比较耗时，最终造成从库和主库延迟变长。

			存在数据不一致的情况，基于性能的考虑，主库的写入流程并没有等待主从同步完成就会返回结果，那么在极端的情况下，
			比如说主库上 binlog 还没有来得及刷新到磁盘上就出现了磁盘损坏或者机器掉电，就会导致 binlog 的丢失，
			最终造成主从数据的不一致。不过，这种情况出现的概率很低，对于互联网的项目来说是可以容忍的。

			做了主从复制之后，我们就可以在写入时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响到读请求的执行。
			同时呢，在读流量比较大的情况下，我们可以部署多个从库共同承担读流量，这就是所说的“一主多从”部署方式，
			在你的垂直电商项目中就可以通过这种方式来抵御较高的并发读流量。另外，从库也可以当成一个备库来使用，以避免主库故障导致数据丢失；

			当写的时候，从库还没有同步完成，解决数据不一致的方法：
			核心思想就是尽量不去从库中查询信息，纯粹以上面的例子来说，我就有三种解决方案：
				a.第一种方案是数据的冗余。比如发微博，你可以在发送消息队列时不仅仅发送微博 ID，而是发送队列处理机需要的所有微博信息，借此避免从数据库中重新查询数据。
				b.第二种方案是使用缓存。我可以在同步写数据库的同时，也把微博的数据写入到 redis 缓存里面，这样队列处理机在获取微博信息的时候会优先查询缓存，
				  这样也可以保证数据的一致性。
				c.查询主库。我可以在队列处理机中不查询从库而改为查询主库。不过，这种方式使用起来要慎重，要明确查询的量级不会很大，是在主库的可承受范围之内，否则会对主库造成比较大的压力。

				一般推荐前面两种方法，对于延迟需要做好告警告机制，及时修复数据一致性；

		2.如何连接呢？
			单独部署的代理层方案，这一类方案代表比较多，如早期阿里巴巴开源的 Cobar，基于 Cobar 开发出来的 Mycat，360 开源的 Atlas，美团开源的基于 Atlas 开发的 DBProxy 等等。这一类中间件部署在独立的服务器上，业务代码如同在使用单一数据库一样使用它，实际上它内部管理着很多的数据源，当有数据库请求时，
			它会对 SQL 语句做必要的改写，然后发往指定的数据源；
			它一般使用标准的 MySQL 通信协议，所以可以很好地支持多语言。由于它是独立部署的，所以也比较方便进行维护升级，比较适合有一定运维能力的大中型团队使用。它的缺陷是所有的 SQL 语句都需要跨两次网络：从应用到代理层和从代理层到数据源，所以在性能上会有一些损耗。

# 写入数据增加时，如何分库分表

	数据库的写入请求量大造成的性能和可用性方面的问题，要解决这些问题，你所采取的措施就是对数据进行分片。
	这样可以很好地分摊数据库的读写压力，也可以突破单机的存储瓶颈，而常见的一种方式是对数据库做“分库分表”。

	如何对数据库做垂直拆分？
		分库分表是一种常见的将数据分片的方式，它的基本思想是依照某一种策略将数据尽量平均地分配到多个数据库节点或者多个表中。

		不同于主从复制时数据是全量地被拷贝到多个节点，分库分表后，每个节点只保存部分的数据，这样可以有效地减少单个数据库节点和单个数据表中存储的数据量，
		在解决了数据存储瓶颈的同时也能有效地提升数据查询的性能。同时，因为数据被分配到多个数据库节点上，那么数据的写入请求也从请求单一主库变成了请求多个数据分片节点，
		在一定程度上也会提升并发写入的性能。

		数据库分库分表的方式有两种：一种是垂直拆分，另一种是水平拆分。这两种方式，在我看来，掌握拆分方式是关键，理解拆分原理是内核，需要结合自身业务来选择不同的方案；
		垂直拆分：
			将数据库的表拆分到多个不同的数据库中；垂直拆分的原则一般是按照业务类型来拆分，核心思想是专库专用，将业务耦合度比较高的表拆分到单独的库中。
			存在的问题，比如微博关系量早已经过了千亿，单一的数据库或者数据表已经远远不能满足存储和查询的需求了，
			这个时候，你需要将数据拆分到多个数据库和数据表中，也就是对数据库和数据表做水平拆分了。
		水平拆表：
			和垂直拆分的关注点不同，垂直拆分的关注点在于业务相关性，而水平拆分指的是将单一数据表按照某一种规则拆分到多个数据库和多个数据表中，关注点在数据的特点。

			水平拆表的规则：
			1.按照某一个字段的哈希值做拆分，这种拆分规则比较适用于实体表，
				比如说用户表，内容表，我们一般按照这些实体表的 ID 字段来拆分。
				比如说我们想把用户表拆分成 16 个库，每个库是 64 张表，那么可以先对用户 ID 做哈希，哈希的目的是将 ID 尽量打散，然后再对 16 取余，
				这样就得到了分库后的索引值；对 64 取余，就得到了分表后的索引值。
			2.按照某一个字段的区间来拆分；
				比较常用的是时间字段。你知道在内容表里面有“创建时间”的字段，而我们也是按照时间来查看一个人发布的内容。
				我们可能会要看昨天的内容，也可能会看一个月前发布的内容，这时就可以按照创建时间的区间来分库分表
				比如说可以把一个月的数据放入一张表中，这样在查询时就可以根据创建时间先定位数据存储在哪个表里面，再按照查询条件来查询。
				一般来说，列表数据可以使用这种拆分方式，比如一个人一段时间的订单，一段时间发布的内容。
				但是这种方式可能会存在明显的热点，这很好理解嘛，你当然会更关注最近我买了什么，发了什么，所以查询的 QPS 也会更多一些，对性能有一定的影响。
				另外，使用这种拆分规则后，数据表要提前建立好，否则如果时间到了 2020 年元旦，DBA（Database Administrator，数据库管理员）却忘记了建表，
				那么 2020 年的数据就没有库表可写了，就会发生故障了。

		分库分表带来的问题：

			1.数据库在分库分表之后，数据的访问方式也有了极大的改变，原先只需要根据查询条件到从库中查询数据即可，现在则需要先确认数据在哪一个库表中，
			再到那个库表中查询数据。这种复杂度也可以通过数据库中间件来解决；

			2.另外一个问题是一些数据库的特性在实现时可能变得很困难。比如说多表的 JOIN 在单库时是可以通过一个 SQL 语句完成的，但是拆分到多个数据库之后就无法跨库执行 SQL 了，
			不过好在我们对于 JOIN 的需求不高，即使有也一般是把两个表的数据取出后在业务代码里面做筛选，复杂是有一些，不过是可以实现的。
			再比如说在未分库分表之前查询数据总数时只需要在 SQL 中执行 count() 即可，现在数据被分散到多个库表中，我们可能要考虑其他的方案，
			比方说将计数的数据单独存储在一张表中或者记录在 Redis 里面。


		分库分表的好处：
			虽然分库分表会对我们使用数据库带来一些不便，但是相比它所带来的扩展性和性能方面的提升，我们还是需要做的，因为，经历过分库分表后的系统，才能够突破单机的容量和请求量的瓶颈，
			就比如说，电商系统，它正是经历了分库分表，才会解决订单表数据量过大带来的性能衰减和容量瓶颈。

		分库分表的原则：
			1.如果在性能上没有瓶颈点那么就尽量不做分库分表；
			2.如果要做，就尽量一次到位，比如说 16 库，每个库 64 表就基本能够满足几年内你的业务的需求
			3.很多的 NoSQL 数据库，例如 Hbase，MongoDB 都提供 auto sharding 的特性，如果你的团队内部对于这些组件比较熟悉，有较强的运维能力，
			  那么也可以考虑使用这些 NoSQL 数据库替代传统的关系型数据库

# 分库分表的id全局唯一性

	在单库单表的场景下，我们可以使用数据库的自增字段作为 ID，因为这样最简单，对于开发人员来说也是透明的。但是当数据库分库分表后，使用自增字段就无法保证 ID 的全局唯一性了。
	想象一下，当我们分库分表之后，同一个逻辑表的数据被分布到多个库中，这时如果使用数据库自增字段作为主键，那么只能保证在这个库中是唯一的，无法保证全局的唯一性。
	那么假如你来设计用户系统的时候，使用自增 ID 作为用户 ID，就可能出现两个用户有两个相同的 ID，这是不可接受的，那么你要怎么做呢？我建议你搭建发号器服务来生成全局唯一的 ID。

	1.Snowflake 算法来生成业务需要的ID的，依赖系统时间，一旦系统时间不准确，可能存在重复的ID
		Snowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID；
		Snowflake 算法设计得非常简单且巧妙，性能上也足够高效，同时也能够生成具有全局唯一性、单调递增性和有业务含义的 ID，
		但是它也有一些缺点，其中最大的缺点就是它依赖于系统的时间戳，一旦系统时间不准，就有可能生成重复的 ID。
		所以如果我们发现系统时钟不准，就可以让发号器暂时拒绝发号，直到时钟准确为止。

	2.UUID（Universally Unique Identifier，通用唯一标识码）不依赖于任何第三方系统，效率比较高
		存在的问题：UUID 是由 32 个 16 进制数字组成的字符串，如果作为数据库主键使用比较耗费空间
				  插入的时候，数据的顺序无法得到保证，数据离散，占据的空间比较大，影响插入和更新性能
				  依赖本地机器的mac地址，一旦mac地址泄漏了，可能存在一定的安全风险；

	3.发号服务，比如 使用独立服务的方式来部署发号器的，性能上单实例单 CPU 可以达到两万每秒
		存在的问题： 发号器每毫秒只发一个 ID，就会造成生成 ID 的末位永远是 1，那么在分库分表时如果使用 ID 作为分区键就会造成库表分配的不均匀


# 数据库和NoSQL如何互补

	将传统的关系型数据改成分布式存储服务，一般从2个方面进行：
	1.提升读写性能，尤其是读性能，大多数的产品服务都是读多写少，比如微博系统，查询的qps远远要大于写入qps
	2.增强存储的拓展能力，从而应对大数据量的存储需求；

	对于一些数量上亿的数据量，即使采用分库分表，每个表的数据千万级的量，数据量的增长，系统很快就会碰到瓶颈，加索引优化表似乎没有作用；
	对于NoSQL来说，有较强的横向拓展能力和读写性能，非常契合高并发的项目；
		a.比如redis,leveldb这样的k/v存储，比传统的db，读写能力强；
		b.比如hbase,hive,spark这样的列式存储数据库，非常适合离线数据统计的场景;
		c.像mongodb,couchdb文档型数据库，模式自由，数据库表中的字段可以自由拓展
		比如电商系统中的商品有很多属性，不同品类的商品字段也不同，使用关系型数据可能要更多的表或加字段来解决，不好拓展和维护；
	nosql的优势：
		1.弥补了传统数据库在性能方面的不足；
		2.数据库变更方便，不需要更改原先的数据结构；
		3.适合互联网项目常见的大数据量的场景；
		4.很多 NoSQL 数据库都在使用的基于 LSM 树的存储引擎；
	这种看法是个误区，因为慢慢地我们发现在业务开发的场景下还是需要利用 SQL 语句的强大的查询功能以及传统数据库事务和灵活的索引等功能，NoSQL 只能作为一些场景的补充。

	对于传统的关系型的数据库存在的问题：
	MySQL 的 InnoDB 存储引擎来说，更新 binlog、redolog、undolog 都是在做顺序 IO，而更新 datafile 和索引文件则是在做随机 IO，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后批量刷新到磁盘上，但是随机 IO 还是会发生
	MySQL 主键是聚簇索引（一种索引类型，数据与索引数据放在一起），既然数据和索引数据放在一起，那么在数据插入或者更新的时候，我们需要找到要插入的位置，
	再把数据写到特定的位置上，这就产生了随机的 IO。而且一旦发生了页分裂，就不可避免会做数据的移动，也会极大地损耗写入性能。

	比如语句“select * from product where name like ‘% 电冰箱’”就没有使用到字段“name”上的索引，而“select * from product where name like ‘索尼 %’”就使用了“name”上的索引。而一旦没有使用索引就会扫描全表的数据，在性能上是无法接受的。

	使用开源组件 Elasticsearch 来支持搜索的请求，它本身是基于“倒排索引”来实现的，那么什么是倒排索引呢？
	倒排索引是指将记录中的某些列做分词，然后形成的分词与记录 ID 之间的映射关系；
	将电冰箱的产品的ID 采用索引行的方式实现，比如1,2,3这样处理，当查询电冰箱的时候，就可以快速找到1,2,3
	以倒排索引作为核心技术原理，为你提供了分布式的全文搜索服务，这在传统的关系型数据库中使用 SQL 语句是很难实现的;

	nosql可以提升拓展性
	比如mongodb的优势：
	1.支持副本集，数据多份，可以保证系统高可用，同时可以分担读请求；
	主节点负责写入，并且将数据记录到oplog里（类似mysql binlog),从节点接收到oplog后会和主节点保持数据一致；
	一旦主节点挂了，立马会从节点中选择一个节点，成为主节点，一般建议mongodb 一主两从或多个从节点模式；
	2.分片，可以理解为mysql的分库分表，将数据按照某种规则拆分成多份，存在不同的机器上；
	  mongodb分片一般需要3个角色来支持，一个shard server,实际存储数据节点，是一个独立的mongod进程，二是config server，是一组mongod进程，存放元信息，比如哪些分片存放了什么数据；
	  三是route server不存数据，仅作为路由分发使用；
	  如何工作呢？route server从config server 获得元信息后，将上游的请求转发给shard server 实际存放数据的节点上；
	3.负载均衡，mongodbk可以自动发现shard server的数据分布，如果不均匀会启动balancer进程对数据调整，从而达到均匀分布；同时数据会自动扩容处理，当shard server存储的空间不足时候，
	  会将数据自动移动到新的shard server上；
	NoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好的补充

	对于nosql来说，弥补关系型数据库在性能、扩展性和某些场景下的不足，所以你在使用或者选择时要结合自身的场景灵活地运用。

	使用 Elasticsearch 作为持久存储，支撑社区的 feed 流功能，初期开发的时候确实很爽，你可以针对 feed 中的任何字段做灵活高效地查询，
	业务功能迭代迅速，代码也简单易懂。可是到了后期流量上来之后，由于缺少对于 Elasticsearch 成熟的运维能力，造成故障频出，尤其到了高峰期就会出现节点不可用的问题，
	而由于业务上的巨大压力又无法分出人力和精力对 Elasticsearch 深入的学习和了解，最后不得不做大的改造到mysql或mongodb上来；

	小结： 对于开源组件的NOSQL产品和mysql，要结合具体业务场景，选择合适的存储引擎，以及对它们有足够的把控能力；

# 缓存

	缓存是一种存储数据的组件，它的作用是让对数据的请求更快地返回。

	缓存的分类：
		静态缓存、分布式缓存和热点本地缓存

		静态缓存： 比如页面静态化,html文档，在 Nginx 上部署静态缓存可以减少对于后台应用服务器的压力；
		分布式缓存：比如redis,mc性能强劲,内存数据库，读写能力强
		热点本地缓存： 主要部署在应用服务器的代码中，用于阻挡热点查询对于分布式缓存节点或者数据库的压力；

	缓存的主要作用是提升访问速度，从而能够抗住更高的并发；
	
	存在的不足：
		1.缓存比较适合于读多写少的业务场景，并且数据最好带有一定的热点属；
		2.缓存会给整体系统带来复杂度，并且会有数据不一致的风险；
		  当更新数据库成功，更新缓存失败的场景下，缓存中就会存在脏数据。对于这种场景，我们可以考虑使用较短的过期时间或者手动清理的方式来解决。
		3.受限于内存的大小，数据量大了，可能撑爆内存
		4.缓存会给运维也带来一定的成本，定位问题有时候比较耗时；
	如何决策呢？
		a.缓存对于性能的提升是毋庸置疑的，我们在做架构设计的时候也需要把它考虑在内，
		只是在做具体方案的时候需要对缓存的设计有更细致的思考，才能最大化地发挥缓存的优势。

		b.缓存可以有多层，比如上面提到的静态缓存处在负载均衡层，分布式缓存处在应用层和数据库层之间，本地缓存处在应用层。
		我们需要将请求尽量挡在上层，因为越往下层，对于并发的承受能力越差；

		c.缓存命中率是我们对于缓存最重要的一个监控项，越是热点的数据，缓存的命中率就越高;

		d.缓存不仅仅是一种组件的名字，更是一种设计思想，你可以认为任何能够加速读请求的组件和设计方案都是缓存思想的体现;

# 缓存读写策略选择

	1.Cache Aside（旁路缓存）策略
		当先更新数据库记录，再更新缓存数据，很容易造成数据不一致的情况。
		主要是因为请求可能是并发同时请求过来比如多个人同时都操作了db和redis cache，就可能由于时间前后，导致cache里的数据被覆盖；
		如何解决呢？ cache aside策略，在更新数据时不更新缓存，而是删除缓存中的数据，在读取数据时，发现缓存中没了数据之后，再从数据库中读取数据，更新到缓存中。
		这个策略数据以数据库中的数据为准，缓存中的数据是按需加载的。它可以分为读策略和写策略；
			读策略：当用户请求的时候，没有命中缓存，就会从db中捞数据，然后回填到cache中；
			写策略：更新数据库中的记录，删除掉缓存；

		cache aside也会存在数据不一致的场景，比如用户a请求未命中缓存，从db捞数据后，回填到cache中，这个时候用户b更新了数据，
		然后删除了cache;这时候a把之前读到的数据更新到db，就会出现覆盖b用户修改的数据，造成db数据和cache不一致问题；
		发生的概率比较小，因为没有删除了cache后，数据为空，可以从db捞数据，回写到cache的时候，可以设置一个短暂的过期时间，避免了数据不一致；

		如果你的业务对缓存命中率有严格的要求，那么可以考虑两种解决方案：
			1.一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，只允许一个请求操作cache；
			2.在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快过期，对业务的影响也是可以接受；
    
    2.Read/Write Through（读穿 / 写穿）策略

    	核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据；

    	Write Through 的策略：
    		先查询要写入的数据在缓存中是否已经存在，如果已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中；
    		如果缓存中数据不存在，我们把这种情况叫做“Write Miss（写失效）；
    		
    		一般来说，我们可以选择两种“Write Miss”方式：一个是“Write Allocate（按写分配）”，做法是写入缓存相应位置，
    		再由缓存组件同步更新到数据库中；另一个是“No-write allocate（不按写分配）”，做法是不写入缓存中，而是直接更新到数据库中。

    		在 Write Through 策略中，我们一般选择“No-write allocate”方式，原因是无论采用哪种“Write Miss”方式，
    		我们都需要同步将数据更新到数据库中，而“No-write allocate”方式相比“Write Allocate”还减少了一次缓存的写入，能够提升写入的性能。
    	
    	Read Through 策略：
    		先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库中同步加载数据；

    	Read Through/Write Through 策略的特点是由缓存节点而非用户来和数据库打交道，在我们开发过程中相比 Cache Aside 策略要少见一些，
    	原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库，或者自动加载数据库中的数据的功能，
    	而我们在使用本地缓存的时候可以考虑使用这种策略；

    3.Write Back（写回）策略
    	核心思想是在写入数据时只写入缓存，并且把缓存块儿标记为“脏”的。而脏块儿只有被再次使用时才会将其中的数据写入到后端存储中。

    一般来说，Cache Aside 是我们在使用分布式缓存时最常用的策略；Read/Write Through 和 Write Back 策略需要缓存组件的支持，所以比较适合你在实现本地缓存组件的时候使用；

# 缓存高可用选择

	选择的方案有客户端方案、中间代理层方案和服务端方案三大类；

	客户端方案： 在客户端配置多个缓存的节点，通过缓存写入和读取算法策略来实现分布式，从而提高缓存的可用性；读方面一般采用主从和多副本两种策略，两种策略是为了解决不同的问题而提出的。
			   缓存数据如何分片？
			   单一的缓存节点受到机器内存、网卡带宽和单节点请求量的限制，不能承担比较高的并发。
			   因此我们考虑将数据分片，依照分片算法将数据打散到多个不同的节点上，每个节点上存储部分数据。
			   分片算法常见的就是 Hash 分片算法和一致性 Hash 分片算法两种。
			   Hash 分片的算法： 
			   		就是对缓存的 Key 做哈希计算，然后对总的缓存节点个数取余。比如采用crc32(name) % n方式，将数据打散到不同的分片上；
			   		缺点：当增加或者减少缓存节点时，缓存总的节点个数变化造成计算出来的节点发生变化，从而造成缓存失效不可用；可以采用另外的方法实现兜底；
			   一致性 Hash 算法：
			   		将整个 Hash 值空间组织成一个虚拟的圆环，然后将缓存节点的 IP 地址或者主机名做 Hash 取值后，放置在这个圆环上；
			   		当我们需要确定某一个 Key 需要存取到哪个节点上的时候，先对这个 Key 做同样的 Hash 取值，
			   		确定在环上的位置，然后按照顺时针方向在环上“行走”，遇到的第一个缓存节点就是要访问的节点。
			   		缺点：缓存节点在圆环上分布不平均，会造成部分缓存节点的压力较大；当某个节点故障时，
			   		     这个节点所要承担的所有访问都会被顺移到另一个节点上，会对后面这个节点造成压力。
			   		     一致性 Hash 算法的脏数据问题，当服务出现异常的时候，客户端可能读到的节点数据分布在不同的位置上；
			   		所以，在使用一致性 Hash 算法时一定要设置缓存的过期时间，这样当发生漂移时，之前存储的脏数据可能已经过期，就可以减少存在脏数据的几率；

	中间代理层： 在应用代码和缓存节点之间增加代理层，客户端所有的写入和读取的请求都通过代理层，而代理层中会内置高可用策略，帮助提升缓存系统的高可用；
	服务端方案： Redis 2.4 版本后提出的 Redis Sentinel 方案和Redis cluster集群方案；

	如何选择？
	分布式缓存的三种方案各有所长，有些团队可能在开发过程中已经积累了 Smart Client 上的一些经验；
	而有些团队在 Redis 运维上经验丰富，就可以推进 Sentinel 方案；
	有些团队在存储研发方面有些积累，就可以推进中间代理层方案，甚至可以自研适合自己业务场景的代理层组件，具体的选择还是要看团队的实际情况而定。

# 缓存穿透解决方案

	回种空值以及使用布隆过滤器。
	1.回填空值，并设置短暂的过期时间。
		从数据库中查询到空值或者发生异常时，我们可以向缓存中回种一个空值。
		但是因为空值并不是准确的业务数据，并且会占用缓存的空间，
		所以我们会给这个空值加一个比较短的过期时间，让空值在短时间之内能够快速过期淘汰；
		对于极热点缓存数据穿透，可以通过设置分布式锁或者后台线程定时加载的方式来解决；

	2.使用布隆过滤器
		把集合中的每一个值按照提供的 Hash 算法算出对应的 Hash 值，然后将 Hash 值对数组长度取模后得到需要计入数组的索引值，
		并且将数组这个位置的值从 0 改成 1。在判断一个元素是否存在于这个集合中时，你只需要将这个元素按照相同的算法计算出索引值，
		如果这个位置的值为 1 就认为这个元素在集合中，否则则认为不在集合中。
		缺点： 
			它在判断元素是否在集合中时是有一定错误几率的，比如它会把不是集合中的元素判断为处在集合中；不支持删除元素；
			当布隆过滤器判断元素在集合中时，这个元素可能不在集合中。但是一旦布隆过滤器判断这个元素不在集合中时，它一定不在集合中；


# mq消息队列
	
	目的：异步处理，削峰填谷，服务解耦，使系统模块之间松耦合，服务高可用；
	也就是说它可以削平短暂的流量高峰，虽说堆积会造成请求被短暂延迟处理，但是只要我们时刻监控消息队列中的堆积长度，
	在堆积量超过一定量时，增加队列处理机数量来提升消息的处理能力就好了，比如秒杀的用户对于短暂延迟知晓秒杀的结果也是有一定容忍度的。

	异步处理、解耦合和削峰填谷是消息队列在秒杀系统设计中起到的主要作用，其中异步处理可以简化业务流程中的步骤，提升系统性能；
	削峰填谷可以削去到达秒杀系统的峰值流量，让业务逻辑的处理更加缓和；
	解耦合可以将秒杀系统和数据系统解耦开，这样两个系统的任何变更都不会影响到另一个系统。

# 消息投递怎么保证只消费一次
	
	消息丢失的可能的三种情况：
		1.消息从生产者写入到消息队列的过程；
		2.消息在消息队列中的存储场景；
		3.消息被消费者消费的过程；


	1. 在消息生产的过程中丢失消息
		消息的生产者一般是我们的业务服务器，消息队列是独立部署在单独的服务器上的。
		两者之间的网络虽然是内网但是也会存在抖动的可能，而一旦发生抖动，消息就有可能因为网络的错误而丢失；
		对于这种情况，建议消息重传，也就是当你发现发送超时后就将消息重新发一次，但也不能无限制地重传消息。
		一般来说，如果不是消息队列发生故障或者是到消息队列的网络断开了，重试 2～3 次就可以了。

		不过这种方案可能会造成消息的重复，从而在消费的时候重复消费同样的消息。
		比方说消息生产时由于消息队列处理慢或者网络的抖动，导致虽然最终写入消息队列成功但在生产端却超时了，生产者重传这条消息就会形成重复的消息。
	
	2. 在消息队列中丢失消息

		拿 Kafka 举例，消息在 Kafka 中是存储在本地磁盘上的，而为了减少消息存储时对磁盘的随机 I/O，我们一般会将消息先写入到操作系统的 Page Cache 中，
		然后再找合适的时机刷新到磁盘上。比如 Kafka 可以配置当达到某一时间间隔或者累积一定的消息数量的时候再刷盘，也就是所说的异步刷盘。
		不过如果发生机器掉电或者机器异常重启，Page Cache 中还没有来得及刷盘的消息就会丢失了。那么怎么解决呢？
		你可能会把刷盘的间隔设置很短或者设置累积一条消息就就刷盘，但这样频繁刷盘会对性能有比较大的影响，而且从经验来看，
		出现机器宕机或者掉电的几率也不高，所以我不建议你这样做。

		如果你的系统对消息丢失的容忍度很低，你可以考虑以集群方式部署 Kafka 服务，通过部署多个副本备份数据保证消息尽量不丢失。
		kafka如何保证消息不丢失呢？
		Kafka 集群中有一个 Leader 负责消息的写入和消费，可以有多个 Follower 负责数据的备份。Follower 中有一个特殊的集合叫做 ISR（in-sync replicas），
		当 Leader 故障时，新选举出来的 Leader 会从 ISR 中选择，默认 Leader 的数据会异步地复制给 Follower，这样在 Leader 发生掉电或者宕机时，
		Kafka 会从 Follower 中消费消息，减少消息丢失的可能。

		由于默认消息是异步地从 Leader 复制到 Follower 的，所以一旦 Leader 宕机，那些还没有来得及复制到 Follower 的消息还是会丢失。
		为了解决这个问题，Kafka 为生产者提供一个选项叫做“acks”，当这个选项被设置为“all”时，生产者发送的每一条消息除了发给 Leader 外还会发给所有的 ISR，
		并且必须得到 Leader 和所有 ISR 的确认后才被认为发送成功。这样，只有 Leader 和所有的 ISR 都挂了消息才会丢失。

	3. 在消费的过程中存在消息丢失的可能

		以kafka为例，一个消费者消费消息的进度是记录在消息队列集群中的，而消费的过程分为三步：接收消息、处理消息、更新消费进度。
		接收消息和处理消息的过程都可能会发生异常或者失败，比如消息接收时网络发生抖动，导致消息并没有被正确地接收到；
		处理消息时可能发生一些业务的异常导致处理流程未执行完成，这时如果更新消费进度，这条失败的消息就永远不会被处理了，也可以认为是丢失了。

		所以，在这里你需要注意的是，一定要等到消息接收和处理完成后才能更新消费进度，但是这也会造成消息重复的问题，
		比方说某一条消息在处理之后消费者恰好宕机了，那么因为没有更新消费进度，所以当这个消费者重启之后还会重复地消费这条消息。


	为了避免消息丢失我们需要付出两方面的代价：一方面是性能的损耗，一方面可能造成消息重复消费；

	性能的损耗我们还可以接受，因为一般业务系统只有在写请求时才会有发送消息队列的操作，而一般系统的写请求的量级并不高，
	但是消息一旦被重复消费就会造成业务逻辑处理的错误。那么我们要如何避免消息的重复呢？

	想要完全地避免消息重复的发生是很难做到的，因为网络的抖动、机器的宕机和处理的异常都是比较难以避免的，在工业上并没有成熟的方法，
	因此我们会把要求放宽，只要保证即使消费到了重复的消息，从消费的最终结果来看和只消费一次是等同的就好了，也就是保证在消息的生产和消费的过程是“幂等”的。

	幂等是一个数学上的概念，它的含义是多次执行同一个操作和执行一次操作，最终得到的结果是相同的；
	你可以这么理解“幂等”：一件事儿无论做多少次都和做一次产生的结果是一样的，那么这件事儿就具有幂等性。

	在生产、消费过程中增加消息幂等性的保证
		消息在生产和消费的过程中都可能会产生重复，所以你要做的是在生产过程和消费过程中增加消息幂等性的保证，这样就可以认为从“最终结果上来看”消息实际上是只被消费了一次的。
		
		在消息生产过程中，在 Kafka0.11 版本和 Pulsar 中都支持“producer idempotency”的特性，翻译过来就是生产过程的幂等性，
		这种特性保证消息虽然可能在生产端产生重复，但是最终在消息队列存储时只会存储一份。

		它的做法是给每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一 ID，消息队列的服务端会存储 < 生产者 ID，最后一条消息 ID> 的映射。
		当某一个生产者产生新的消息时，消息队列服务端会比对消息 ID 是否与存储的最后一条 ID 一致，如果一致就认为是重复的消息，服务端会自动丢弃。


		在消费端，幂等性的保证会稍微复杂一些，你可以从通用层和业务层两个层面来考虑。
			在通用层面，你可以在消息被生产的时候使用发号器给它生成一个全局唯一的消息 ID，消息被处理之后把这个 ID 存储在数据库中，
			在处理下一条消息之前先从数据库里面查询这个全局 ID 是否被消费过，如果被消费过就放弃消费。

		无论是生产端的幂等性保证方式还是消费端通用的幂等性保证方式，它们的共同特点都是为每一个消息生成一个唯一的 ID，然后在使用这个消息的时候，
		先比对这个 ID 是否已经存在，如果存在则认为消息已经被使用过。所以这种方式是一种标准的实现幂等的方式，你在项目之中可以拿来直接使用。
		不过这样也存在一些问题：
			如果消息在处理之后，还没有来得及写入数据库，消费者宕机了重启之后发现数据库中并没有这条消息，还是会重复执行两次消费逻辑，
			这时你就需要引入事务机制，保证消息处理和写入数据库必须同时成功或者同时失败，但是这样消息处理的成本就更高了，所以如果对于消息重复没有特别严格的要求，
			可以直接使用这种通用的方案，而不考虑引入事务。
		在业务层面怎么处理呢？
		这里有很多种处理方式，其中有一种是增加乐观锁的方式。比如你的消息处理程序需要给一个人的账号加钱，那么你可以通过乐观锁的方式来解决。

# 如何降低消息队列的延迟

	监控消息延迟
		1.使用消息队列提供的工具，通过监控消息的堆积来完成；
		2.通过生成监控消息的方式来监控消息的延迟情况；

	减少消息延迟的正确姿势：
		想要减少消息的处理延迟，我们需要在消费端和消息队列两个层面来完成。
			1.优化消费代码提升性能；
			2.增加消费者的数量，加强消费者消费的能力，避免消息堆积；

	对于消息队列中有一段时间没有新的消息，于是消费客户端拉取不到新的消息就会不间断地轮询拉取消息，导致机器实例cpu暴涨；
	所以，你在写消费客户端的时候要考虑这种场景，拉取不到消息可以等待一段时间再来拉取，等待的时间不宜过长，否则会增加消息的延迟。
	我一般建议固定的 10ms~100ms，也可以按照一定步长递增，比如第一次拉取不到消息等待 10ms，第二次 20ms，最长可以到 100ms，直到拉取到消息再回到 10ms。

	说完了消费端的做法之后，再来说说消息队列本身在读取性能优化方面做了哪些事情？
	主要从两方面考虑读取性能问题：
		1.消息的存储
			有时候，在设计的时候为了实现简单，使用了普通的数据库来存储消息，但是受限于数据库的性能瓶颈，读取 QPS 只能到 2000，
			后面我重构了存储模块，使用本地磁盘作为存储介质。Page Cache 的存在就可以提升消息的读取速度，即使要读取磁盘中的数据，
			由于消息的读取是顺序的并且不需要跨网络读取数据，所以读取消息的 QPS 提升了一个数量级。
		2.零拷贝技术
			说是零拷贝，其实我们不可能消灭数据的拷贝，只是尽量减少拷贝的次数。在读取消息队列的数据的时候，
			其实就是把磁盘中的数据通过网络发送给消费客户端，在实现上会有四次数据拷贝的步骤：
				1.数据从磁盘拷贝到内核缓冲区
				2.系统调用将内核缓存区的数据拷贝到用户缓冲区
				3.用户缓冲区的数据被写入到 Socket 缓冲区中
				4.操作系统再将 Socket 缓冲区的数据拷贝到网卡的缓冲区中
			经过上面4个步骤后，将消息通过网络发送给消费客户端，从而实现零拷贝；
		操作系统提供了 Sendfile 函数可以减少数据被拷贝的次数。使用了 Sendfile 之后，在内核缓冲区的数据不会被拷贝到用户缓冲区而是直接被拷贝到 Socket 缓冲区，
		节省了一次拷贝的过程提升了消息发送的性能。高级语言中对于 Sendfile 函数有封装，比如说在 Java 里面的 java.nio.channels.FileChannel 类就提供了
		transferTo 方法提供了 Sendfile 的功能。

	如何提升消息队列的性能来降低消息消费的延迟，归结为3点：
		a.使用消息队列提供的工具，或者通过发送监控消息的方式来监控消息的延迟情况;
		b.横向扩展消费者是提升消费处理能力的重要方式;
		c.选择高性能的数据存储方式配合零拷贝技术，可以提升消息的消费性能;

# 微服务拆分的原则
	
	1.做到单一服务内部功能的高内聚和低耦合
		也就是说每个服务只完成自己职责之内的任务，对于不是自己职责的功能交给其它服务来完成。
		说起来你可能觉得理所当然对这一点不屑一顾，但很多人在实际开发中，经常会出现一些问题。
	2.你需要关注服务拆分的粒度，先粗略拆分再逐渐细化。
		在服务拆分的初期，你其实很难确定服务究竟要拆分成什么样。但是从“微服务”这几个字来看，服务的粒度貌似应该足够小，
		甚至有“一方法一服务”的说法。不过服务多了也会带来问题，像是服务个数的增加会增加运维的成本。
		再比如原本一次请求只需要调用进程内的多个方法，现在则需要跨网络调用多个 RPC 服务，在性能上肯定会有所下降
		推荐做法：
			拆分初期可以把服务粒度拆得粗一些，后面随着团队对于业务和微服务理解的加深，再考虑把服务粒度细化。
			比如对于一个社区系统来说，你可以先把和用户关系相关的业务逻辑，都拆分到用户关系服务中，之后，再把比如黑名单的逻辑独立成黑名单服务。
	3.拆分的过程，要尽量避免影响产品的日常功能迭代
		要一边做产品功能迭代，一边完成服务化拆分。
		服务剥离原则：
			1.优先剥离比较独立的边界服务（比如短信服务、地理位置服务），从非核心的服务出发减少拆分对现有业务的影响，也给团队一个练习、试错的机会；
			2.当两个服务存在依赖关系时优先拆分被依赖的服务。
				比如内容服务依赖于用户服务获取用户的基本信息，
			  	那么如果先把内容服务拆分出来，内容服务就会依赖于一体化架构中的用户模块，这样还是无法保证内容服务的快速部署能力。
			  	所以正确的做法是理清服务之间的调用关系，比如内容服务会依赖用户服务获取用户信息，互动服务会依赖内容服务，
			  	所以要按照先用户服务再内容服务，最后互动服务的顺序来进行拆分。
	4.服务接口的定义要具备可扩展性
		服务拆分之后，由于服务是以独立进程的方式部署，所以服务之间通信就不再是进程内部的方法调用而是跨进程的网络通信了。
		在这种通信模型下服务接口的定义要具备可扩展性，否则在服务变更时会造成意想不到的错误。

# 微服务化带来的问题和解决思路
	
	1.服务接口的调用不再是同一进程内的方法调用而是跨进程的网络调用，这会增加接口响应时间的增加，需要引入服务注册中心
		注册中心管理的是服务完整的生命周期，包括对于服务存活状态的检测；
	2.多个服务之间有着错综复杂的依赖关系；
		一个服务会依赖多个其它服务也会被多个服务所依赖，那么一旦被依赖的服务的性能出现问题产生大量的慢请求，
		就会导致依赖服务的工作线程池中的线程被占满，依赖的服务也会出现性能问题。

		为了避免发生这种情况，我们需要引入服务治理体系针对出问题的服务采用熔断、降级、限流、超时控制的方法，使问题被限制在单一服务中，
		保护服务网络中的其它服务不受影响。
	3.服务拆分到多个进程后，一条请求的调用链路上涉及多个服务，那么一旦这个请求的响应时间增长或者是出现错误，我们就很难知道是哪一个服务出现的问题
		另外，整体系统一旦出现故障，很可能外在的表现是所有服务在同一时间都出现了问题，你在问题定位时很难确认哪一个服务是源头，
		这就需要引入分布式追踪工具，以及更细致的服务端监控报表。
	4.微服务化的一个目标是减少研发的成本，其中也包括沟通的成本，所以小团队内部成员不宜过多；
		按照亚马逊 CEO 贝佐斯的“两个披萨”的理论，如果两个披萨不够你的团队吃，那么你的团队就太大了需要拆分，
		所以一个小团队包括开发、运维、测试以 6～8 个人为最佳；
	5.如果你的团队人数不多还没有做好微服务化的准备，而你又感觉到研发和部署的成本确实比较高，那么一个折中的方案是你可以优先做工程的拆分，然后做服务的拆分；

# 压力测试
	
	压力测试指的是在高并发大流量下进行的测试，测试人员可以通过观察系统在峰值负载下的表现，从而找到系统中存在的性能隐患。

	压力测试是一种常见的发现系统中存在问题的方式，也是保障系统可用性和稳定性的重要手段。而在压力测试的过程中，我们不能只针对某一个核心模块来做压测，
	而需要将接入层、所有后端服务、数据库、缓存、消息队列、中间件以及依赖的第三方服务系统及其资源，都纳入压力测试的目标之中。
	因为，一旦用户的访问行为增加，包含上述组件服务的整个链路都会受到不确定的大流量的冲击。
	因此，它们都需要依赖压力测试来发现可能存在的性能瓶颈，这种针对整个调用链路执行的压力测试也称为“全链路压测”。

	一般来说全链路压测平台需要包含以下几个模块:
		1.流量构造和产生模块；
		2.压测数据隔离模块；
		3.系统健康度检查和压测流量干预模块。
	压测数据的产生:
		一般来说，我们系统的入口流量是来自于客户端的 HTTP 请求。所以，我们会考虑在系统高峰期时，将这些入口流量拷贝一份，
		在经过一些流量清洗的工作之后（比如过滤一些无效的请求），将数据存储在像是 HBase、MongoDB 这些 NoSQL
		存储组件或者亚马逊 S3 这些云存储服务中，我们称之为流量数据工厂。
		另外，我们还需要对压测流量染色，也就是增加压测标记。在实际项目中，我会在 HTTP 的请求头中增加一个标记项，
		比如说叫做 is stress test，在流量拷贝之后，批量在请求中增加这个标记项，再写入到数据流量工厂中。

	数据如何隔离?
		将压测流量拷贝下来的同时，我们也需要考虑对系统做改造，以实现压测流量和正式流量的隔离，这样一来就会尽量避免压测对线上系统的影响。
		一般来说，我们需要做两方面的事情。
		1.针对读取数据的请求（一般称之为下行流量），我们会针对某些不能压测的服务或者组件，做 Mock 或者特殊的处理.
			举个例子,在业务开发中，我们一般会依据请求记录用户的行为，比如，用户请求了某个商品的页面，我们会记录这个商品多了一次浏览的行为，
			这些行为数据会写入一份单独的大数据日志中，再传输给数据分析部门，形成业务报表给到产品或者老板做业务的分析决策。
			在压测的时候，肯定会增加这些行为数据，比如原本一天商品页面的浏览行为是一亿次，而压测之后变成了十亿次，这样就会对业务报表产生影响，
			影响后续的产品方向的决策。因此，我们对于这些压测产生的用户行为做特殊处理，不再记录到大数据日志中。

			所以，我们需要 Mock 这些推荐服务，让不带有压测标记的请求经过推荐服务，而让带有压测标记的请求经过 Mock 服务。
			搭建 Mock 服务，你需要注意一点：这些 Mock 服务最好部署在真实服务所在的机房，这样可以尽量模拟真实的服务部署结构，提高压测结果的真实性。
		2.针对写入数据的请求（一般称之为上行流量），我们会把压测流量产生的数据写入到影子库，也就是和线上数据存储完全隔离的一份存储系统中。
		  针对不同的存储类型，我们会使用不同的影子库的搭建方式。
		  a.如果数据存储在 MySQL 中，我们可以在同一个 MySQL 实例，不同的 Schema 中创建一套和线上相同的库表结构，并且把线上的数据也导入进来;
		  b.而如果数据是放在 Redis 中，我们对压测流量产生的数据，增加一个统一的前缀，存储在同一份存储中
		  c.还有一些数据会存储在 Elasticsearch 中，针对这部分数据，我们可以放在另外一个单独的索引表中;

	压力测试如何实施?
		在拷贝了线上流量和完成了对线上系统的改造之后，我们就可以进行压力测试的实施了。在此之前，一般会设立一个压力测试的目标
		比如说，整体系统的 QPS 需要达到每秒 20 万。
		
		在压测时，不会一下子把请求量增加到每秒 20 万次，而是按照一定的步长（比如每次压测增加一万 QPS），逐渐地增加流量。
		在增加一次流量之后，让系统稳定运行一段时间，观察系统在性能上的表现。
		如果发现依赖的服务或者组件出现了瓶颈，可以先减少压测流量，比如，回退到上一次压测的 QPS，保证服务的稳定，
		再针对此服务或者组件进行扩容，然后再继续增加流量压测。

		为了能够减少压力测试过程中人力投入成本，可以开发一个流量监控的组件，在这个组件中，预先设定一些性能阈值。
		比如，容器的 CPU 使用率的阈值可以设定为 60%～70%；系统的平均响应时间的上限可以设定为 1 秒；系统慢请求的比例设置为 1% 等等。

		当系统性能达到这个阈值之后，流量监控组件可以及时发现，并且通知压测流量下发组件减少压测流量，并且发送报警给到开发和运维的同学，
		开发和运维同学就迅速地排查性能瓶颈，在解决问题或者扩容之后再继续执行压测。

	压力测试的原则：
		1.压力测试是一种发现系统性能隐患的重要手段，所以应该尽量使用正式的环境和数据；
		2.对压测的流量需要增加标记，这样就可以通过 Mock 第三方依赖服务和影子库的方式来实现压测数据和正式数据的隔离；
		3.压测时，应该实时地对系统性能指标做监控和告警，及时地对出现瓶颈的资源或者服务扩容，避免对正式环境产生影响；

	全链路压力测试带来的价值：
		1.它可以帮助我们发现系统中可能出现的性能瓶颈，方便我们提前准备预案来应对；
		2.它也可以为我们做容量评估，提供数据上的支撑；
		3.我们也可以在压测的时候做预案演练，因为压测一般会安排在流量的低峰期进行，这样我们可以降级一些服务来验证预案效果，并且可以尽量减少对线上用户的影响；

# 服务监控
	
	服务监控的目的：
		1.第一时间感知线上问题，及时发现问题并快速解决；
		2.用最小的成本将线上影响降低到最低，保证系统的稳定性和可用性；

	监控指标如何选择？
		在搭建监控系统时，所面临的第一个问题就是选择什么样的监控指标，也就是监控什么。有些同学在给一个新的系统设定监控指标的时候会比较迷茫，
		不知道从哪方面入手。其实，有一些成熟的理论和套路你可以直接拿来使用。比如，谷歌针对分布式系统监控的经验总结，四个黄金信号（Four Golden Signals）。
		它指的是在服务层面一般需要监控四个指标，分别是延迟、通信量、错误和饱和度。

		1.延迟指的是请求的响应时间。比如接口的响应时间、访问数据库和缓存的响应时间
		2.通信量可以理解为吞吐量，也就是单位时间内请求量的大小。比如访问第三方服务的请求量，访问消息队列的请求量。
		3.错误表示当前系统发生的错误数量。
			这里需要注意的是， 我们需要监控的错误既有显式的，比如在监控 Web 服务时，出现 4 * * 和 5 * * 的响应码；也有隐式的，
			比如 Web 服务虽然返回的响应码是 200，但是却发生了一些和业务相关的错误（出现了数组越界的异常或者空指针异常等），这些都是错误的范畴。
		4.饱和度指的是服务或者资源到达上限的程度；
			也可以说是服务或者资源的利用率，比如 CPU 的使用率、内存使用率、磁盘使用率、缓存数据库的连接数等等；

	服务端监控搭建的过程，需要注意的点：
		a.耗时、请求量和错误数是三种最通用的监控指标；
		b.Agent、埋点和日志是三种最常见的数据采集方式;
		c.访问趋势报表用来展示服务的整体运行情况，性能报表用来分析资源或者依赖的服务是否出现问题，资源报表用来追查资源问题的根本原因。
		这三个报表共同构成了你的服务端监控体系。
	总之，监控系统是你发现问题，排查问题的重要工具，你应该重视它，并且投入足够的精力来不断地完善它。
	只有这样，才能不断地提高对系统运维的掌控力，降低故障发生的风险。

